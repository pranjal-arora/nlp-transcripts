# -*- coding: utf-8 -*-
"""PranjalArora_102003402_NLP in Python - 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EV7bg55DTTz7hFAwbb4UpPNINgUNzSQr

# Exploratory Data Analysis

## Introduction

After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.

When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. We are going to look at the following for each comedian:

1. **Most common words** - find these and create word clouds
2. **Size of vocabulary** - look number of unique words and also how quickly someone speaks
3. **Amount of profanity** - most common terms

## Most Common Words

### Analysis
"""

import pandas as pd

# Read in the document-term matrix (word counts in matrix format)
data = pd.read_pickle('/content/sample_data/dtm.pkl')  #previosuly pickled file
data

data = data.transpose() #moving the rows data to the column and columns data to the rows.
data.head()

# Find the top 30 words said by each comedian

top_dict = {}  #empty dictioanry
for c in data.columns:
    top = data[c].sort_values(ascending=False).head(30) #column wise sorting first 30 most-occuring words
    top_dict[c]= list(zip(top.index, top.values))  #zip() method maps elements from two tuples that have same index, and maps them into a dictionary.
    #If the passed iterators have different lengths, the iterator with the least items decides the length of the new dictionary.
    #zip() method :- takes iterables(tuples/lists/strings) or containers
    #zip() method:- returns a single iterator object that has mapped values in form of dictionary

top_dict

# Print the top 15 words said by each comedian

for comedian, top_words in top_dict.items():
    print(comedian)
    print(', '.join([word for word, count in top_words[0:14]])) #print top 15 words of each comedian
    print('---')

"""**NOTE:** At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.


"""

# "Collections" module provides with specialized container datatypes, thus providing an alternative to dictionaries, lists, and tuples. 
# These specialised "containers" hold data objects, and provide a way to access the contained objects, and iterate over them.
# "Counter" is a container included in the collections module. Counter is a sub-class that is used to count hashable objects. 
#It implicitly creates a hash table of an iterable when invoked.
from collections import Counter

#here we are creating a list of words from top_dict

# Look at the most common top words --> add them to the stop word list

# Let's first pull out the top 30 words for each comedian
words = []
for comedian in data.columns:
    top = [word for (word, count) in top_dict[comedian]]
    for t in top:
        words.append(t)
        
words

# Let's aggregate this list and identify the most common words along with how many routines they occur in
Counter(words).most_common()
#Counter has a most_common() method that returns a list of tuples of (element, count) sorted by counts.
# Returns highest number of occurrences by specifying the index as [0] , the one with the lowest as [-1] , and so on

#12 out of 12 comedians are speaking the word 'like'

# If more than half of the comedians have it as a top word, exclude it from the list
add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]  #more than 6 comedians say this word
add_stop_words

# Let's update our document-term matrix with the new list of stop words

from sklearn.feature_extraction import text 
from sklearn.feature_extraction.text import CountVectorizer 
#CountVectorizer-It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.
#It is a method to convert text to numerical data. 

# Read in cleaned data
data_clean = pd.read_pickle('/content/sample_data/data_clean.pkl')

# Add new stop words
stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words) #The union() method returns a set that contains all items from the original set, 
# and all items from the specified set(s), here it is our new list "add_stop_words"

# Recreate document-term matrix
cv = CountVectorizer(stop_words=stop_words) #with new stop words
data_cv = cv.fit_transform(data_clean.transcript)
data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())
data_stop.index = data_clean.index

# Pickle it for later use
import pickle
pickle.dump(cv, open("cv_stop.pkl", "wb"))
data_stop.to_pickle("dtm_stop.pkl")

data_cv

data_stop

# Let's make some word clouds!

# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud

#Word Cloud is a data visualization technique used for representing text data in 
#which the size of each word indicates its frequency or importance. Significant 
#textual data points can be highlighted using a word cloud. 
from wordcloud import WordCloud

wc = WordCloud(stopwords=stop_words, background_color="white", colormap="Dark2",
               max_font_size=150, random_state=42) #by setting random_state parameter, you ensure reproducibility of the exact same word cloud.

# Reset the output dimensions


#matplotlib.pyplot is a collection of functions that make matplotlib work like 
#MATLAB. Each pyplot function makes some change to a figure: e.g., creates a 
#figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.

import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = [16, 6] 
#Each time Matplotlib loads, it defines a runtime configuration (rc) containing the 
#default styles for every plot element you create. This configuration can be adjusted 
#at any time using the plt.rc convenience routine. rcParams are the parameters that can be set in the .matplotlibrc file

full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',
              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']


# Create subplots for each comedian
for index, comedian in enumerate(data.columns): #enumerate() method adds counters to list
    wc.generate(data_clean.transcript[comedian]) #generate wordcloud
    
    plt.subplot(3, 4, index+1) #draw multiple plots in one figure
    plt.imshow(wc, interpolation="bilinear") #imshow() to show data as an image, interpolation= Bilinear Interpolation : is a resampling method that uses 
    #the distanceweighted average of the four nearest pixel values to estimate a new pixel value. The four cell centers from
    #the input raster are closest to the cell center for the output processing cell will be weighted and based on distance and then averaged.
    #interpolation default value is antialiased
    plt.axis("off") #remove axis
    plt.title(full_names[index])  #adds names of the comedian on their respective wordcloud
    
plt.show() #display figures

"""### Findings

* Ali Wong says the s-word a lot and talks about her husband. I guess that's funny to me.
* A lot of people use the F-word. Let's dig into that later.

## Number of Words

### Analysis
"""

# Find the number of unique words that each comedian uses

# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once
unique_list = []
for comedian in data.columns:
    uniques = data[comedian].to_numpy().nonzero()[0].size
    unique_list.append(uniques)

# Create a new dataframe that contains this unique word count
data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])
data_unique_sort = data_words.sort_values(by='unique_words')
data_unique_sort #unique words and their count in descending order

# Calculate the words per minute of each comedian

# Find the total number of words that a comedian uses
total_list = []
for comedian in data.columns:
    totals = sum(data[comedian])
    total_list.append(totals)
    
# Comedy special run times from IMDB, in minutes
run_times = [60, 59, 80, 60, 67, 73, 77, 63, 62, 58, 76, 79]

# Let's add some columns to our dataframe
data_words['total_words'] = total_list
data_words['run_times'] = run_times
data_words['words_per_minute'] = data_words['total_words'] / data_words['run_times']

# Sort the dataframe by words per minute to see who talks the slowest and fastest
data_wpm_sort = data_words.sort_values(by='words_per_minute')
data_wpm_sort

# Let's plot our findings

#Here plt. barh(courses, values, color='maroon') is used to 
#specify that the bar chart is to be plotted by using the courses column as the Y-axis, and the values as the X-axis. 
#yticks() function takes a list object as argument. The elements in the list denote the positions on corresponding action where ticks will be displayed.
#Ticks are the markers denoting data points on axes.
import numpy as np

y_pos = np.arange(len(data_words)) #Return evenly spaced values within a given interval. 

#UNIQUE WORDS
plt.subplot(1, 2, 1)
plt.barh(y_pos, data_unique_sort.unique_words, align='center') #plt. barh(courses, values, color='maroon') is used to specify that the bar 
#chart is to be plotted by using the courses column as the Y-axis, and the values as the X-axis. 
plt.yticks(y_pos, data_unique_sort.comedian) # yticks() function takes a list object as argument. The elements in the list denote the 
#positions on corresponding action where ticks will be displayed. Ticks are the markers denoting data points on axes.
plt.title('Number of Unique Words', fontsize=20)

#WORDS PER MINUTE
plt.subplot(1, 2, 2)
plt.barh(y_pos, data_wpm_sort.words_per_minute, align='center')
plt.yticks(y_pos, data_wpm_sort.comedian)
plt.title('Number of Words Per Minute', fontsize=20)

plt.tight_layout()
plt.show()

"""### Findings

* **Vocabulary**
   * Ricky Gervais (British comedy) and Bill Burr (podcast host) use a lot of words in their comedy
   * Louis C.K. (self-depricating comedy) and Anthony Jeselnik (dark humor) have a smaller vocabulary


* **Talking Speed**
   * Joe Rogan (blue comedy) and Bill Burr (podcast host) talk fast
   * Bo Burnham (musical comedy) and Anthony Jeselnik (dark humor) talk slow
   
Ali Wong is somewhere in the middle in both cases. Nothing too interesting here.

## Amount of Profanity

### Analysis
"""

#Let's take a look at the most common words again. #next we'll look into profanity/obscene words
Counter(words).most_common()

# Let's isolate just these bad words
data_bad_words = data.transpose()[['fucking', 'fuck', 'shit']]
data_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)
data_profanity.columns = ['f_word', 's_word']
data_profanity

# Let's create a scatter plot of our findings
plt.rcParams['figure.figsize'] = [10, 8]

for i, comedian in enumerate(data_profanity.index):
    x = data_profanity.f_word.loc[comedian]
    y = data_profanity.s_word.loc[comedian]
    plt.scatter(x, y, color='blue')
    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)
    plt.xlim(-5, 155) 
    
plt.title('Number of Bad Words Used in Routine', fontsize=20)
plt.xlabel('Number of F Bombs', fontsize=15)
plt.ylabel('Number of S Words', fontsize=15)

plt.show()

"""### Findings

* **Averaging 2 F-Bombs Per Minute!** - I don't like too much swearing, especially the f-word, which is probably why I've never heard of Bill Bur, Joe Rogan and Jim Jefferies.
* **Clean Humor** - It looks like profanity might be a good predictor of the type of comedy I like. Besides Ali Wong, my two other favorite comedians in this group are John Mulaney and Mike Birbiglia.

## Side Note

What was our goal for the EDA portion of our journey? **To be able to take an initial look at our data and see if the results of some basic analysis made sense.**

My conclusion - yes, it does, for a first pass. There are definitely some things that could be better cleaned up, such as adding more stop words or including bi-grams. But we can save that for another day. The results, especially the profanity findings, are interesting and make general sense, so we're going to move on.

As a reminder, the data science process is an interative one. It's better to see some non-perfect but acceptable results to help you quickly decide whether your project is a dud or not, instead of having analysis paralysis and never delivering anything.

**Alice's data science (and life) motto: Let go of perfectionism!**

## Additional Exercises

1. What other word counts do you think would be interesting to compare instead of the f-word and s-word? Create a scatter plot comparing them.
"""

#additonal
#BAD WORDS
data_bad_words = data.transpose()[['gay','ass','loser','asshole','nigga']]
data_profanity = pd.concat([data_bad_words.ass+data_bad_words.asshole, data_bad_words.gay+data_bad_words.loser+data_bad_words.nigga], axis = 1)
data_profanity.columns = ['a_words','n_words']
data_profanity

#create a scatter plot of this

plt.rcParams['figure.figsize'] = [10,8]

for i, comedian in enumerate(data_profanity.index):
    x = data_profanity.a_words.loc[comedian]
    y = data_profanity.n_words.loc[comedian]
    plt.scatter(x,y, color="blue")
    plt.text(x+1.5, y+0.5, full_names[i], fontsize = 10)
    plt.xlim(-5,155)

plt.title('Number of Bad Words Used in Routine', fontsize = 20)
plt.xlabel('Number of A Words', fontsize = 15)
plt.ylabel('Number of N Words', fontsize = 15)

plt.show()

#FINDINGS
#dave chapelle uses the most n words
#jim jefferies uses the most a words